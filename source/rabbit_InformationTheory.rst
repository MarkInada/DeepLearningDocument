
情報理論
========

目標：情報理論を理解する
~~~~~~~~~~~~~~~~~~~~~~~~

◼︎自己情報量
~~~~~~~~~~~~

確率 P(>0)
で起こる事象を観測したときに得られる自己情報量を以下のように定義する。（情報量を考える際に対数の底は
2 を用いることが多い。底を 2
で考えるとき，情報量の単位は「1bit」または「1シャノン」です。）

.. math::  I(x) = - \log _2 (P(x)) = \log _2 (W(x)) 

普段コンピュータで表す2進数のbitは底が2の対数であらわす。以下の値のことをAの自己情報量と言う。

.. math::  \log _2 A 

掛け算で表される事象の量を足し算で表せるようにするために対数を用いて以下のように表す。

.. math::  A = 2^a 

.. math::  B = 2^b 

という値がある時、A × B は、

.. math::  A × B = 2^a × 2^b = 2^{a+b} 

と表される。これを底が2の対数に当てはめると、

.. math::  \log _2 (A × B) = a + b = \log _2 A + \log _2 B 

というように元々積を求める計算だった計算が和を求める計算になった。

また、参考までに逆数は以下のように計算できるので、

.. math::  \frac{1}{A} = \frac{1}{2^a} = 2^{-a} 

以下のように逆数の対数の場合はマイナスの対数と一致する。

.. math::  \log \frac{1}{A} = -\log A 

つまり、自己情報量で使われるW(x)はP(x)の逆数になっている。

.. math::  W(x) = \frac{1}{P(x)} 

◼︎情報量を理解する
~~~~~~~~~~~~~~~~~~

金魚が100匹入っている水槽に新たに1匹の金魚を入れても多くの人は金魚が増えたことに気づかない。しかし、元々1匹だけ入っている水槽に新たに金魚を入れると多くの人は金魚が増えたことに気づく。これは後者の方が情報量が大きいから。これを数式であらわすと以下のようになる。

.. math::  \frac{ΔW(新たに増えた事象の数)}{W(元々の事象の数)} = ΔI(増えた情報) 

100匹の水槽に1匹入れる場合に増える情報は0.01。1匹の水槽に1匹入れる場合は1になる。

上記の式を変換すると、Δを微小な値dとして以下のようにも表せる。

.. math::  dI = \frac{dW}{W} 

これを積分すると量になるので以下のように計算できる。

.. math::  \int dI = \int \frac{dW}{W} 

.. math::  I = \int \frac{1}{W}dW 

.. math::  = \log W 

◼︎自己情報量の例
~~~~~~~~~~~~~~~~

「（公平な）コインを投げて表が出た」ことを観測したときに得る情報量は

.. math::  \log _2 \frac{1}{2} = 1 bit 

「友人が，確率 0.000002
で三億円当たるような宝くじで三億円当てた」ことを観測したときに得る情報量は

.. math::  \log _2 0.00002 := 19.9 bit 

コインで表が出ることの情報量1bitに対し、宝くじが当たる情報量はとても大きい数字になっていることがわかる。

◼︎シャノンエントロピー
~~~~~~~~~~~~~~~~~~~~~~

・エントロピーは複雑さの意味

・自己情報量の期待値（平均値）・・・以下の式で説明すると、Iの平均をとったもの。

.. math::  H(x) = E(I(x)) 

.. math::  = -E(\log P(x)) 

.. math::  = -\sum_{k=1}^{n} (P(x_k)\log P(x_k)) 

.. code:: ipython3

    import numpy as np
    from scipy.stats import entropy
    
    # サイコロのシャノンエントロピーを求めてみる
    test_entropy = 0
    for i in range(6):
        test_entropy += 1/6. * np.log(1/6.)
        
    print("求めたシャノンエントロピー: ", test_entropy*-1)
    
    dice = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])
    print("ライブラリで確認: ", entropy(dice))


.. parsed-literal::

    求めたシャノンエントロピー:  1.7917594692280547
    ライブラリで確認:  1.791759469228055


.. code:: ipython3

    # いかさまサイコロの場合、普通のサイコロの方がエントロピーが低い（複雑さが低い）
    
    bad_dice = np.array([1/12, 1/3, 1/18, 1/2, 1/6, 1/6])
    print("イカサマサイコロのエントロピー: ", entropy(bad_dice))


.. parsed-literal::

    イカサマサイコロのエントロピー:  1.55166214857287


◼︎Kullback-Leibler divergence ( KLダイバージェンス、KL情報量 )
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

・同じ事象・確率変数における異なる確率分布P, Qの違いを表す

.. math::  D_{KL}(P||Q) = \sum_{i} P(x_i)\log \frac{P(x_i)}{Q(x_i)} 

.. math::  = \sum_{i} P(x_i) (\log P(x_i) - \log Q(x_i)) 

PとQのどちらで平均をとればいいか？前提として先に古い情報から事象Qを予想しているというイメージがある。例えば、あるニュースの注目度をQと予想していたとする。しかし、アンケート結果で実際にはPであることが薄々わかってきたとする。この場合、Pから予想していたQを引くことでどれだけニュースの注目度が減っていくのかがわかる。元々Qという予想に対してPが変わっていくことの変化量がKLダイバージェンス。従ってPについて考えたいので、定義はPで平均をとる式になっていることがわかる。

例) 芸能人のピエール瀧が薬物を使ったニュースの注目度ln
Qを"１０"と予想した。しかし、SNSの調査結果から音楽家でもある芸能人が薬物を使う注目度ln
Pは１ヶ月後には"5", ３ヶ月後には"3"
しかないことがわかってきた。この場合、KLダイバージェンスは１ヶ月後より３ヶ月後の方が大きい(PとQとの違いが大きい)ことがわかってくる。

◼︎KLダイバージェンスを使ってみよう！
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ピエール瀧氏の薬物ニュースを100年に一度の大ニュースだと予測したとする。しかし、SNSの調査で半年後には2年に一度は音楽やってる人は薬物使うよね、という印象になった瀧事件のKLダイバージェンスは、およそ0.58。対して、ベッキー氏とゲス川谷氏の不倫も1000年に一度の大ニュースだと予測したとする。これは予想に従って半年後も100年以上に一度のニュースだということがSNSの調査でわかったとする。このKLダイバージェンスは、およそ0.007。予想と現実の差が激しい方がKLダイバージェンスが大きいことが確認できる。

.. code:: ipython3

    import numpy as np
    from scipy.stats import entropy
    
    # ピエール瀧の薬物ニュース
    predict_taki = np.array([1/100,1/100,1/100,1/100,1/100])
    actual_taki = np.array([1/100,1/10,1/5,1/3,1/2])
    
    # ベッキーとゲス川谷の不倫
    predict_bekey = np.array([1/100,1/100,1/100,1/100,1/100])
    actual_bekey = np.array([1/100,1/110,1/120,1/130,1/140])
    
    print("ピエール瀧の薬物ニュース: ", entropy(predict_taki, actual_taki))
    print("ベッキーとゲス川谷の不倫: ", entropy(predict_bekey, actual_bekey))


.. parsed-literal::

    ピエール瀧の薬物ニュース:  0.5863005922044968
    ベッキーとゲス川谷の不倫:  0.007097628155427107


◼︎交差エントロピー
~~~~~~~~~~~~~~~~~~

・KLダイバージェンスの一部を取り出したもの

上記の通り、KLダイバージェンスは事象Pについての平均をとっていた。そこで以下のように、定義式内の事象Pの情報量の平均をとるようになっている箇所は、ただの情報量を表しているにすぎない。

.. math::  \sum_{i}　P(x_i) \log P(x_i) 

そこで、この情報量を省き、Qにのみ着目した定義式を交差エントロピーと言い、Qについての自己情報量をPの分布で平均した式になる。Qに関することをPで平均をとっているので交差と言う。

.. math::  交差エントロピーH(P, Q) = - \sum_{i} P(x_i) \log Q(x_i) 

少ない情報しか送れない、とてもトラフィックがNarrowな無線通信を行う状況を想像する。そんな時、Qという小さな情報しか送れないことを予想する。Qは天気の情報、時間の情報、気温の情報・・など不要な情報か必要な情報かをかなり精査した情報になる。この情報Qは、実際に送りたいと思っている情報Pとはかけ離れている。事前に考えていた情報Qが実際に送らないといけないPと、どれだけ噛み合っていたかを考えるのが交差エントロピー。PとQに差がなければ差がないほど予想と現実にギャップが少なく、交差エントロピーが小さくなる。

◼︎交差エントロピーを使ってみよう！
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

いかさまコインを使ったベルヌーイ分布に従う確率分布 P(x),
Q(x)について考える。ただし、検証者はイカサマと知らないとする。コインAの表が出る確率μは0.3、コインBは0.2、コインCは0.1、とする。

まず、検証者が普通のコインでコイントスする場合は以下のように予想Pのμ=0.5と現実Qのμ=0.5となる。

.. math::  普通のコインの交差エントロピーH(P, Q) = - \sum_{i} P(x_i) \log Q(x_i) 

.. math::  = - ((0.5 × \log 0.5) + (0.5 × \log 0.5)) 

.. math::  := 0.69 

次に、検証者がいかさまコインAでコイントスする場合は以下のように予想Pのμ=0.5と現実Qのμ=0.7となる。

.. math::  コインAの交差エントロピーH(P, Q) = - \sum_{i} P(x_i) \log Q(x_i) 

.. math::  = - ((0.7 × \log 0.5) + (0.3 × \log 0.5)) 

.. math::  := 0.78 

この通り、推定が外れるほど交差エントロピーは大きくなります。

.. code:: ipython3

    #　交差エントロピーの検証
    
    import numpy as np
    from scipy.stats import entropy
    
    coin = np.array([0.5, 0.5])
    coinA = np.array([0.3, 0.7])
    coinB = np.array([0.2, 0.8])
    coinC = np.array([0.1, 0.9])
    
    coin_entropy = 0
    coinA_entropy = 0
    coinB_entropy = 0
    coinC_entropy = 0
    
    for i in range(2):
        coin_entropy += coin[i] * np.log(coin[i])
        coinA_entropy += coin[i] * np.log(coinA[i])
        coinB_entropy += coin[i] * np.log(coinB[i])
        coinC_entropy += coin[i] * np.log(coinC[i])
        
    print("普通のコイン: ", coin_entropy*-1)
    print("普通のコインとコインAの交差エントロピー: ", coinA_entropy*-1)
    print("普通のコインとコインBの交差エントロピー: ", coinB_entropy*-1)
    print("普通のコインとコインCの交差エントロピー: ", coinC_entropy*-1)


.. parsed-literal::

    普通のコイン:  0.6931471805599453
    普通のコインとコインAの交差エントロピー:  0.7803238741323343
    普通のコインとコインBの交差エントロピー:  0.916290731874155
    普通のコインとコインCの交差エントロピー:  1.203972804325936


◼︎補足
~~~~~~

exp: 指数関数 ex のこと

ln: 自然対数 logex のこと

lg: 2を底とする対数 log2x のこと（を表す場合が多い

◼︎引用, 参考資料
~~~~~~~~~~~~~~~~

・ラビットチャレンジ - 応用数学講座

http://ai999.careers/rabbit/

・東京大学グローバル消費インテリジェンス寄付講座 - Data Science Online
Course

https://gci.t.u-tokyo.ac.jp/

・Qiita - 正規分布間のKLダイバージェンス

https://qiita.com/ceptree/items/9a473b5163d5655420e8

・Qiita - 生成モデルで語られる Kullback-Leibler を理解する

https://qiita.com/TomokIshii/items/b9a11c19bd5c36ad0287

・Qiita - エントロピー・KL divergenceの復習

https://qiita.com/kento1109/items/10026d96f2634ba36362

・scipy.org

https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html
